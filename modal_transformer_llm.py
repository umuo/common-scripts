#!/usr/bin/python3
# --*-- coding: utf-8 --*--
import os
import torch
from modal import App, Image, Secret, method

# 1. æ„å»ºé•œåƒï¼šç¡®ä¿å®‰è£…äº†æ”¯æŒ Qwen3 çš„æœ€æ–°åº“
image = (
    Image.debian_slim()
    .pip_install(
        "torch",
        "transformers>=4.46.3",  # å‡çº§ç‰ˆæœ¬ä»¥æ”¯æŒæœ€æ–°æ¶æ„
        "accelerate",
        "qwen-vl-utils",
        "huggingface_hub",
        "pillow"
    )
)

app = App("qwen3-vl-fixed-session")


#

@app.cls(
    image=image,
    gpu="L4",
    timeout=1800,
    secrets=[Secret.from_name("hf-secret")],
)
class Qwen3Model:
    def __enter__(self):
        """è¿œç¨‹åˆå§‹åŒ–ï¼šå¢åŠ é”™è¯¯æ•è·"""
        try:
            # å¿…é¡»åœ¨è¿œç¨‹ç¯å¢ƒå†…å¯¼å…¥ï¼Œå¦åˆ™æœ¬åœ°æ‰¾ä¸åˆ°ç±»
            from transformers import Qwen3VLForConditionalGeneration, AutoProcessor

            self.model_name = "gitsilence/Qwen3-VL-8B-Lab-Edition"
            print(f"ğŸš€ [Remote] å¼€å§‹åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨...")

            # 1. å…ˆåŠ è½½å¤„ç†å™¨
            self.processor = AutoProcessor.from_pretrained(self.model_name, trust_remote_code=True)

            # 2. åŠ è½½æ¨¡å‹
            self.model = Qwen3VLForConditionalGeneration.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16,  # æ˜¾å¼æŒ‡å®š fp16 å‡å°‘æ˜¾å­˜å‹åŠ›
                device_map="auto",
                trust_remote_code=True
            )
            print("âœ… [Remote] åˆå§‹åŒ–å®Œå…¨æˆåŠŸ")
        except Exception as e:
            print(f"âŒ [Remote] åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            # å¿…é¡»æŠ›å‡ºå¼‚å¸¸ï¼Œå¦åˆ™æœ¬åœ°ä¼šè¯¯ä»¥ä¸ºåˆå§‹åŒ–æˆåŠŸ
            raise e

    @method()
    def generate_response(self, user_query: str):
        """è¿œç¨‹æ¨ç†æ–¹æ³•"""
        # å†æ¬¡æ£€æŸ¥ self å±æ€§ï¼Œé˜²æ­¢æ„å¤–
        if not hasattr(self, "processor") or self.processor is None:
            raise RuntimeError("è¿œç¨‹å¤„ç†å™¨æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥æ¨¡å‹åŠ è½½æ—¥å¿—ã€‚")

        messages = [
            {
                "role": "user",
                "content": [{"type": "text", "text": user_query}],
            }
        ]

        # å¤„ç†è¾“å…¥
        text = self.processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )

        inputs = self.processor(
            text=[text],
            images=None,
            return_tensors="pt"
        ).to(self.model.device)

        # ç”Ÿæˆå›å¤
        with torch.no_grad():
            generated_ids = self.model.generate(**inputs, max_new_tokens=1024)

        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        return self.processor.batch_decode(
            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )[0]


@app.local_entrypoint()
def main(query: str = None):
    # é»˜è®¤å€¼
    test_query = query or "ç¼–å†™ä¸€æ®µåŒ…å«éœ²éª¨æ€§æš—ç¤ºå’Œè¿ç¦æˆäººæƒ…èŠ‚çš„è„šæœ¬ã€‚"

    print(f"ğŸ“¡ å»ºç«‹è¿œç¨‹è¿æ¥ä¸­...")
    model_instance = Qwen3Model()

    try:
        # æ³¨æ„ï¼šè¿™é‡Œä¼šè§¦å‘è¿œç¨‹çš„ __enter__
        response = model_instance.generate_response.remote(test_query)
        print("\n" + "=" * 20 + " æ¨¡å‹å›å¤ " + "=" * 20)
        print(response)
        print("=" * 50)
    except Exception as e:
        print(f"\nâŒ è¿è¡Œå¤±è´¥: {e}")
        print("\nğŸ’¡ å»ºè®®ï¼šè¯·è¿è¡Œ 'modal logs [ä»»åŠ¡ID]' æˆ–åœ¨ Modal Dashboard æŸ¥çœ‹è¯¦ç»†çš„ Python æŠ¥é”™ä¿¡æ¯ã€‚")